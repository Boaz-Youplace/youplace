# -*- coding: utf-8 -*-
"""Boaz_Adv 데이터전처리.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-0kDHVcI3CFsd2vLDvT4p5Lx8kLtGSKk

https://ebbnflow.tistory.com/292

https://www.dinolabs.ai/288

한글 키워드 추출 시스템: 문서 단위
http://nlp.kookmin.ac.kr/cgi-bin/indexT.cgi

youtube api에서 title, description 가져오기
"""

#!/usr/bin/python

from apiclient.discovery import build
from apiclient.errors import HttpError
from oauth2client.tools import argparser
#from Google import Create_Service

# Set DEVELOPER_KEY to the API key value from the APIs & auth > Registered apps
# tab of
#   https://cloud.google.com/console
# Please ensure that you have enabled the YouTube Data API for your project.
DEVELOPER_KEY = "AIzaSyDweYriWH-xpyKILmBNK1fdg-yx4yA6Azg"
YOUTUBE_API_SERVICE_NAME = "youtube"
YOUTUBE_API_VERSION = "v3"

youtube = build(YOUTUBE_API_SERVICE_NAME, YOUTUBE_API_VERSION,developerKey=DEVELOPER_KEY)

search_response = youtube.search().list(
q = "제주 동부 vlog",
order = "date",
part = "snippet",
maxResults = 50
).execute()

# video_response = youtube.videos().list(
# q = "제주 동부 vlog",
# order = "date",
# part = "snippet",
# maxResults = 10
# ).execute()

titles = []
descriptions = []

for i in search_response['items']:
  #print(i)
  titles.append(i['snippet']['title'])
  descriptions.append(i['snippet']['description'])
print("title!!!",titles)
print("description!!!",descriptions)
#print(len(titles), len(descriptions))

# #!/usr/bin/env python
# import re

# # emoji_pattern = re.compile("["
# #         u"\U0001F600-\U0001F64F"  # emoticons
# #         u"\U0001F300-\U0001F5FF"  # symbols & pictographs
# #         u"\U0001F680-\U0001F6FF"  # transport & map symbols
# #         u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
# #                            "]+", flags=re.UNICODE)

# # for i in titles:
# #   print(emoji_pattern.sub(r'', i)) # no emoji

# only_BMP_pattern = re.compile("["
#         u"\U00010000-\U0010FFFF"  #BMP characters 이외
#                            "]+", flags=re.UNICODE)

# for i in  titles:
#   print(only_BMP_pattern.sub(r'', i))# BMP characters만

# # print("titles",titles)

# print(titles[0])
# # 이 방법으론 띄어쓰기도 삭제가 되어버림
# # new_string = ''.join(char for char in titles[0] if char.isalnum())
# # print(new_string)

# def cleanText(readData):
#     #텍스트에 포함되어 있는 특수 문자 제거
#     text = re.sub('[-=+,#/\?:^$.@*\"※~&%ㆍ!』\\‘|\(\)\[\]\<\>`\'…》]|', '', readData)
#     return text
# thisdata = cleanText(titles[0])
# print(thisdata)

"""영어,숫자 및 공백 제거

"""

#!/usr/bin/env python
import re

# 새로운 리스트
clear_titles = []
# 텍스트를 가지고 있는 리스트
for i in titles:
    # 영어,숫자 및 공백 제거.
    text = re.sub('[^a-zA-Z0-9가-힣]',' ',i).strip()
    # 빈 리스트는 제거.
    if(text != ''):
        clear_titles.append(text)
print(titles)
print("##")
print(clear_titles)

# 새로운 리스트
clear_descriptions = []
# 텍스트를 가지고 있는 리스트
for i in descriptions:
    # 영어,숫자 및 공백 제거.
    text = re.sub('[^a-zA-Z0-9가-힣]',' ',i).strip()
    # 빈 리스트는 제거.
    if(text != ''):
        clear_descriptions.append(text)
print(descriptions)
print("##")
print(clear_descriptions)

"""Dataframe 으로 만들기"""

import pandas as pd
import numpy as np

title_df = pd.DataFrame(clear_titles)
title_df.columns = ['title']
description_df = pd.DataFrame(clear_descriptions)
description_df.columns = ['description']

title_df.head()

description_df.head()

df = pd.concat([title_df, description_df], axis = 1)
df

#token화를 위해서 mecab 설치하기
! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git

cd Mecab-ko-for-Google-Colab/

! bash install_mecab-ko_on_colab190912.sh

import konlpy
from konlpy.tag import Mecab

# KoNLPy Komoran 토크나이저 
from konlpy.tag import Komoran 
komoran = Komoran() 
def komoran_tokenizer(sent): 
  words = komoran.pos(sent, join=True) 
  words = [w for w in words if ('/NN' in w or '/XR' in w or '/VA' in w or '/VV' in w)] 
  return words

pip install textrank

pip install textrankr

# 에러남ㅜㅜ
# # TextRank Keyword Extraction 
# from textrank import KeywordSummarizer 
# keyword_summarizer = KeywordSummarizer(tokenize=komoran_tokenizer, min_count=2, min_cooccurrence=1) 
# keyword_summarizer.summarize(cleaned_sents, topk=30)

"""Okt를 통해 명사만 출력해보기"""

from konlpy.tag import Okt
from collections import Counter

nouns = []
okt = Okt()
for c in clear_titles:
  noun = okt.nouns(c)
  for i,v in enumerate(noun):
    if len(v)<2:
      noun.pop(i)
  # print(noun)
  #nouns.append(noun)
  nouns += noun
print(nouns)
count = Counter(nouns)
noun_list = count.most_common(100)
for v in noun_list:
  print(v)

nouns_description = []
okt = Okt()
for c in clear_descriptions:
  noun = okt.nouns(c)
  for i,v in enumerate(noun):
    if len(v)<2:
      noun.pop(i)
  # print(noun)
  #nouns.append(noun)
  nouns_description += noun
print(nouns_description)
count = Counter(nouns_description)
noun_list = count.most_common(100)
for v in noun_list:
  print(v)

print(titles[0])
# 이 방법으론 띄어쓰기도 삭제가 되어버림
new_string = ''.join(char for char in titles[0] if char.isalnum())
print(new_string)

from konlpy.tag import Okt
from collections import Counter

nouns = []
okt = Okt()
for c in clear_titles:
  noun = okt.nouns(c)
  for i,v in enumerate(noun):
    if len(v)<2:
      noun.pop(i)
  # print(noun)
  #nouns.append(noun)
  nouns += noun
print(nouns)
count = Counter(nouns)
noun_list = count.most_common(100)
for v in noun_list:
  print(v)

nouns_descriptrion = []
okt = Okt()
for c in clear_descriptions:
  noun = okt.nouns(c)
  for i,v in enumerate(noun):
    if len(v)<2:
      noun.pop(i)
  # print(noun)
  #nouns.append(noun)
  nouns_descriptrion += noun
print(nouns_descriptrion)
count = Counter(nouns_descriptrion)
noun_list = count.most_common(100)
for v in noun_list:
  print(v)

"""띄어쓰기 없는 문장으로도 되는지 테스트 해보기"""

print(titles[0])
# 이 방법으론 띄어쓰기도 삭제가 되어버림
new_string = ''.join(char for char in titles[0] if char.isalnum())
print(new_string)

test_titles = []
for i in titles:
  new_string = ''.join(char for char in i if char.isalnum())
  test_titles.append(new_string)
print(test_titles)

from konlpy.tag import Okt
from collections import Counter

nouns = []
okt = Okt()
for c in test_titles:
  noun = okt.nouns(c)
  for i,v in enumerate(noun):
    if len(v)<2:
      noun.pop(i)
  # print(noun)
  #nouns.append(noun)
  nouns += noun
print(nouns)
count = Counter(nouns)
noun_list = count.most_common(100)
for v in noun_list:
  print(v)

test_descriptions = []
for i in descriptions:
  new_string = ''.join(char for char in i if char.isalnum())
  test_descriptions.append(new_string)
print(test_descriptions)

nouns_descriptrion = []
okt = Okt()
for c in clear_descriptions:
  noun = okt.nouns(c)
  for i,v in enumerate(noun):
    if len(v)<2:
      noun.pop(i)
  # print(noun)
  #nouns.append(noun)
  nouns_descriptrion += noun
print(nouns_descriptrion)
count = Counter(nouns_descriptrion)
noun_list = count.most_common(100)
for v in noun_list:
  print(v)